base:
  use_cuda: True

data:
  dataset_folder: data
  images_folder: book_cover_img
  all_qa_pairs_file: all_qa_pairs.txt
  train_dataset: train.csv
  eval_dataset: val.csv
  question_col: question
  image_col: image_id
  answer_col: answer
  answer_space: answer_space.txt

tokenizer:
  padding: longest
  max_length: 64
  truncation: True
  return_token_type_ids: True
  return_attention_mask: True

model:
  name: bert-beit  # Custom name for the multimodal model
  text_encoder: bert-base-multilingual-uncased # Valid transformer model for text encoding from HuggingFace
  image_encoder: microsoft/beit-base-patch16-224-pt22k-ft22k  # Valid transformer model for image encoding from HuggingFace
  intermediate_dims: 512
  dropout: 0.3

train:
  output_dir: checkpoint
  seed: 12345
  num_train_epochs: 5
  learning_rate: 5.0e-5
  weight_decay: 0.0
  warmup_ratio: 0.0
  warmup_steps: 0
  evaluation_strategy: steps
  eval_steps: 100
  logging_strategy: steps
  logging_steps: 100
  save_strategy: steps
  save_steps: 100
  save_total_limit: 3            # Save only the last 3 checkpoints at any given time while training 
  metric_for_best_model: wups
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  remove_unused_columns: False
  dataloader_num_workers: 2
  load_best_model_at_end: True

metrics:
  metrics_folder: metrics
  metrics_file: metrics.json

inference:
  checkpoint: checkpoint-1500